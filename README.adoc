= Review of Structure and Interpretation of Computer Programs
:toc:
:toclevels: 4
:toc-placement!:
:source-highlighter: rouge
:stem: latexmath
:imagesdir: assets
:figure-caption!:
ifdef::env-github[]
:imagesdir: https://raw.githubusercontent.com/cj1128/sicp-review/master/assets
endif::[]

My review of the awesome SICP book.

image::cover.jpg[align="center"]

Useful links:

* https://mitpress.mit.edu/sites/default/files/sicp/index.html[Official site for SICP]
* https://www.gnu.org/software/mit-scheme/documentation/stable/mit-scheme-ref.pdf[Mit Scheme Reference]
* https://www.gnu.org/software/mit-scheme/documentation/stable/mit-scheme-user.pdf[Mit Scheme User's Manual]

== Setup

For Mac OS::

. `brew install mit-scheme`
. `brew install rlwrap`
. `cp REPO/scheme-completion.txt ~/scheme-completion.txt`
. `echo 'alias sicp="rlwrap -r -c -f ~/scheme-completion.txt mit-scheme"' >> ~/.zshrc`

If you are using Sublime Text, you can configure the build system:

[source,json]
----
{
  "shell_cmd": "run-mit-scheme $file",
  "file_regex": "^[ ]*File \"(...*?)\", line ([0-9]*)",
  "selector": "source.scheme"
}
----

I wrap `mit-scheme` program in `run-mit-scheme` script to append trailing new line so that the build result looks better in https://packagecontrol.io/packages/BuildX[BuildX].

[source,bash]
----
$ cat run-mit-scheme
#!/bin/bash
result=$(mit-scheme --quiet < $1)
code=$?

echo "$result"
exit $code
----

toc::[]

ifdef::env-github[]
NOTE: Due to the safety reasons, `include::` directive in AsciiDoc will be transformed to `link:` in GitHub. So if you see something like `\link:test.js[]` in the code block, you can manually go check the code in `test.js`.
endif::[]

== Chapter 1: Building Abstractions with Procedures

Lisp is so old and also so good.

[,page 3]
____
Lisp was invented in the late 1950s as a formalism for reasoning about the use of certain kinds of logical expressions, called *recursion equations*, as a model for computation. The language was conceived by John McCarthy and is based on his paper “Recursive Functions of Symbolic Expressions and Their Computation by Machine”.
____

Why SICP chooses lisp?

[,page 5]
____
If Lisp is not a mainstream language, why are we using it as the framework for our discussion of programming? Because the language possesses unique features that make it an excellent medium for studying important programming constructs and data structures and for relating them to the linguistic features that support them.
____

=== 1.1: The Elements of Programming

One of the key features of every programming language is *how to combine simple ideas to form more complex ideas*.

[,page 6]
____
Every powerful language has three mechanisms for accomplishing this:

* *primitive expressions*, which represent the simplest entities the language is concerned with,
* *means of combination*, by which compound elements are built from simpler ones, and
* *means of abstraction*, by which compound elements can be named and manipulated as units.
____

Name matters.

[,page 10]
____
A critical aspect of a programming language is the means it provides for using names to refer to computational objects.
____

Some expressions do not follow general evaluation rule. They are special so they are called `special forms`, like `(define x 1)`.

[,page 14]
____
Such exceptions to the general evaluation rule are called special forms. ... Each special form has its own evaluation rule. The various kinds of expressions (each with its associated evaluation rule) constitute the syntax of the programming language.
____

Lisp has a very simple syntax.

[,page 15]
____
In comparison with most other programming languages, Lisp has a very simple syntax; that is, the evaluation rule for expressions can be described by a simple general rule together with specialized rules for a small number of special forms.
____

Applicative order versus normal order.

[,page 21]
____
This alternative “fully expand and then reduce” evaluation is known as normal-order evaluation , in contrast to the “evaluate the arguments and then apply” method that the interpreter actually uses, which is called applicative-order evaluation.
____

Lisp *uses applicative-order evaluation*.

[,page 21]
____
Lisp uses applicative-order evaluation, partly because of the additional efficiency obtained from avoiding multiple evaluations of expres- sions ... and, more significantly, because normal-order evaluation becomes much more complicated to deal with when we leave the realm of procedures that can be modeled by substitution. On the other hand, normal-order evaluation can be an extremely valuable tool.
____

==== Exercise 1.1

[source, shceme]
----
10 ; 10

(+ 5 3 4) ; 12

(- 9 1) ; 8

(/ 6 2) ; 3

(+ (* 2 4) (- 4 6)) ; 6

(define a 3) ; a

(define b (+ a 1)) ; b

(+ a b (* a b)) ; 19

(= a b) ; #f

(if (and (> b a) (< b (* a b)))
    b
    a) ; 4

(cond
  ((= a 4) 6)
  ((= b 4) (+ 6 7 a))
  (else 25)) ; 16

(+ 2 (if (> b a) b a)) ; 6

(* (cond ((> a b) a)
         ((< a b) b)
         (else -1))
   (+ a 1)) ; 16
----

==== Exercise 1.2

We can clearly see the benefits of the syntax of Lisp. It's so concise yet powerful.

[source,scheme]
----
include::chapter-1/1.1/1.02.scm[]
----

==== Exercise 1.3

[source,scheme]
----
include::chapter-1/1.1/1.03.scm[]
----

==== Exercise 1.4

[source,scheme]
----
(define (a-plus-abs-b a b)
  ((if (> b 0) + -) a b))
----

In this function, the operator itself is a subexpression `(if (> b 0) + -)`. It is either built-in procedure `+` or built-in procedure `-` based on the value of `b`.

In Lisp, operator and operands can be any complex expressions.

==== Exercise 1.5

[source,scheme]
----
(define (p) (p))

(define (test x y)
  (if (= x 0) 0 y))

(test 0 (p))
----

In normal-order evaluator, the program would run normally, return 0. But in applicative-order evaluator, the program would hang forever.

Because in application-order evaluator, when it evaluates `(test 0 (p))`, it will need to evaluate `(p)` first.

`p` is a procedure which calls itself. So to evaluate `(p)`, we get `(p)` again, and we evaluate that, we get it again, so on and so forth, we can never get a result.

But in normal-order evaluator, it won't evaluate operands until it has to. So `(test 0 (p))` becomes `(if (= 0 0) 0 (p)`. Because `(= 0 0)` is true, it will never evaluate `(p)`.

---

How to compute square roots?

[,page 29]
____
How does one compute square roots? Thee most common way is to use Newton’s method of successive approximations, which says that whenever we have a guess y for the value of the square root of a number x , we can perform a simple manipulation to get a better guess (one closer to the actual square root) by averaging y with x/y.
____

NOTE: MIT Scheme, however, distinguishes between exact integers and decimal values, and dividing two integers produces a rational number rather than a decimal.

.Square root procedure
[source,scheme]
----
include::chapter-1/1.1/sqrt.scm[]
----

==== Exercise 1.6

The procedure would hang forever because it continuously evaluates `sqrt-iter`.

==== Exercise 1.7

For small numbers, the result will be inaccurate. Let's say we need to iterate 10 times to find the square root of number x, because x is very small, the first time iterated value and x can satisfy the `good-enough?` condition.

[source,scheme]
----
(display (sqrt 0.00001))
; .03135649010771716
; this value is very inaccurate
----

For big numbers, the program will run forever. We can never find a number to satisfy the condition because we only have limited precision.

[source,scheme]
----
; CAUTION! this function will run forever
(display (sqrt 1.797693134862315708145274237317043567981e+308))
----

Does it help with the new `good-enough?` implementation? Of course!

[source,scheme]
----
include::chapter-1/1.1/1.07.scm[]
----

==== Exercise 1.8

[source,scheme]
----
include::chapter-1/1.1/1.08.scm[]
----

---

We can define functions inside functions.

[quote,page 38]
____
we allow a procedure to have internal definitions that are local to that procedure. Such nesting of definitions, called block structure, is basically the right solution to the simplest name-packaging problem.
____

What is *lexical scoping*?

[quote,page 39]
____
Lexical scoping dictates that free variables in a procedure are taken to refer to bindings made by enclosing procedure definitions; that is, they are looked up in the environment in which the procedure was defined.
____

=== 1.2: Procedures and the Processes They Generate

Recursive process::
+
[source,scheme]
----
(define (factorial n)
  (if (= n 1)
      1
      (* n (factorial (- n 1)))))

(factorial 6)
(* 6 (factorial 5))
(* 6 (* 5 (factorial 4)))
(* 6 (* 5 (* 4 (factorial 3))))
(* 6 (* 5 (* 4 (* 3 (factorial 2)))))
(* 6 (* 5 (* 4 (* 3 (* 2 (factorial 1))))))
(* 6 (* 5 (* 4 (* 3 (* 2 1)))))
(* 6 (* 5 (* 4 (* 3 2))))
(* 6 (* 5 (* 4 6)))
(* 6 (* 5 24))
(* 6 120)
; A linear recursion
----
+
[,page 44]
____
This type of process, characterized by a chain of deferred operations, is called a *recursive process*. Carrying out this process requires that the interpreter keep track of the operations to be performed later on.
____

Iterative process::
+
[source,scheme]
----
(define (factorial n)
  (define (fact-iter product counter max-count)
    (if (> counter max-count)
        product
        (fact-iter (* product counter) (+ counter 1) max-count)))
  (fact-iter 1 1 n))

(factorial 6)
(fact-iter 1 1 6)
(fact-iter 1 2 6)
(fact-iter 2 3 6)
(fact-iter 6 4 6)
(fact-iter 24 5 6)
(fact-iter 120 6 6)
(fact-iter 720 7 6)
----
+
[,page 44]
____
By contrast, the second process does not grow and shrink. At each step, all we need to keep track of, for any n, are the current values of the variables `product`, `counter`, and `max-count`. We call this an *iterative process*.
____

The essential feature of _iterative process_ is that *its state can be summarized by a fixed number of state variables*.

[,page 44]
____
In general, an iterative process is one whose state can be summarized by a fixed number of state variables, together with a fixed rule that describes how the state variables should be updated as the process moves from state to state and an (optional) end test that specifies conditions under which the process should terminate.
____

Pay attention that *a recursive process is not the same as a recursive procedure*. We can use a skill called *Tail Call Optimization* to get an iterative process of a recursive procedure.

In my understanding, process is the running entity and procedure is the static code.

[,page 45]
____
When we describe a procedure as recursive, we are referring to the syntactic fact that the procedure definition refers (either directly or indirectly) to the procedure itself. But when we describe a process as following a pattern that is, say, linearly recursive, we are speaking about how the process evolves, not about the syntax of how a procedure is written.
____

==== Exercise 1.9

[source,scheme]
----
(define (+ a b)
  (if (= a 0) b (inc (+ (dec a) b))))

(+ 4 5)
(inc (+ 3 5))
(inc (inc (+ 2 5)))
(inc (inc (inc (+ 1 5))))
(inc (inc (inc (inc (+ 0 5)))))
(inc (inc (inc (inc 5))))
(inc (inc (inc 6)))
(inc (inc 7))
(inc 8)
9

; this is a recursive process.
----

[source,scheme]
----
(define (+ a b)
  (if (= a 0) b (+ (dec a) (inc b))))

(+ 4 5)
(+ 3 6)
(+ 2 7)
(+ 1 8)
(+ 0 9)
9

; this is an iterative process.
----

==== Exercise 1.10

[source,scheme]
----
include::chapter-1/1.2/1.10.scm[]
----

---

How many different ways can we make change of $1.00, given half-dollars, quarters, dimes, nickels, and pennies?

This problem has a simple solution as a recursive procedure.

.A recursive count-change procedure
[source,scheme]
----
include::chapter-1/1.2/count-change.scm[]
----

It's not obvious how to transform this to an iterative process. I managed to come up with one, but I have to say it's not very straight forward.

.An iterative count-chagne procedure
[source,scheme]
----
include::chapter-1/1.2/count-change-iter.scm[]
----

==== Exercise 1.11

The recursive version is very easy, just follow the formula.

Because this function is a slight modification of `fibonacci`, we can use the same skill to construct the iterative function.

[source,scheme]
----
include::chapter-1/1.2/1.11.scm[]
----

==== Exercise 1.12

It's obvious that `f(row, index) = f(row-1, index-1) + f(row-1, index)`.

[source,scheme]
----
include::chapter-1/1.2/1.12.scm[]
----

==== Exercise 1.13

Given

[stem]
++++
\begin{gathered}
Fib(n) = Fib(n)  + Fib(n-1) \\
\varphi = \frac{1 + \sqrt{5}}{2} \\
\psi = \frac{1 - \sqrt{5}}{2} \\
\end{gathered}
++++

If

[stem]
++++
\begin{gathered}
Fib(n) = \frac{\varphi^n - \psi^n}{\sqrt{5}} \\
Fib(n-1) = \frac{\varphi^{n-1} - \psi^{n-1}}{\sqrt{5}} \\
\end{gathered}
++++

Because

[stem]
++++
\begin{align}
\varphi\psi = -1 \\
\varphi + \psi = 1 \\
\end{align}
++++

We can get

[stem]
++++
\begin{align}
\varphi^{n+1} - \psi^{n+1} & = \varphi\varphi^n - \psi\psi^{n} \\
& = (1 - \psi)\varphi^n - (1 - \varphi)\psi^n \\
& = \varphi^n - \psi\varphi^n - \psi^n + \varphi\psi^n \\
& = \varphi^n - \psi\varphi\varphi^{n-1} - \psi^n + \varphi\psi\psi^{n-1} \\
& = \varphi^n - \psi^n + \varphi^{n-1} - \psi^{n-1} \\

\\

Fib(n+1) & = Fib(n) + Fib(n-1) \\
& = \frac{\varphi^n - \psi^n + \varphi^{n-1} - \psi^{n-1}}{\sqrt{5}} \\
& = \frac{\varphi^{n+1} - \psi^{n+1}}{\sqrt{5}}
\end{align}
++++

---

_Order of growth_ is a convenient way to express how many resources a process needs.

[quote,page 55]
____
Let `n` be a parameter that measures the size of the problem, and let `R(n)` be the amount of resources the process requires for a problem of size `n`.

...

We say that `R(n)` has order of growth `Θ(f(n))`, written `R(n) = Θ(f(n))` (pronounced “theta of f(n)”), if there are positive constants `k1` and `k2` independent of `n` such that `k1f(n) ≤ R(n) ≤ k2f(n)` for any sufficiently largevalueofn.(In other words, for large `n`, the value `R(n)` is sandwiched between `k1f(n)` and `k2f(n)`.)
____

_Order of growth_ provides only a *rough idea*.

[quote,page 56]
____
Orders of growth provide only a crude description of the behavior of a process. For example, a process requiring n^2 steps and a process requiring 1000n^2 steps and a process requiring 3n^2 + 10n + 17 steps all have O(n^2) order of growth.
____


But it still can be very useful.

[quote,page 56]
____
On the other hand, order of growth provides a useful indication of how we may expect the behavior of the process to change as we change the size of the problem.
____

==== Exercise 1.14

The image below is generated by link:chapter-1/1.2/1.14.py[] with Graphviz. Dark node is the leaf node.

image::1.14.png[align="center"]

The space complexity is the depth of the tree, and we can see that is `Θ(n)`.

The time complexity is hard to analyze. The result is `cc(amount, kind) = Θ(amount^kind)`. Here is a https://codology.net/post/sicp-solution-exercise-1-14/[detail explaination].

==== Exercise 1.15

a.::

We need to get sine's argument down to 0.1 by dividing 12.15 by 121.5 or greater number. Every time `p` is applied, argument gets divided by 3. How many times we need to divide 12.15 by 3 to get down to 0.1? We need to find out the x from x^3 = 121.5.
+
[source,scheme]
----
; x^3 = 121.5
; x = log3(121.5) = log(121.5) / log(3)
(display (/ (log 121.5) (log 3))) ; 4.37
; So p will be applied 5 times.
----

b.::

Space and time complexity of `(sine a)` are both `Θ(log3 a)`.

==== Exercise 1.16

[source,scheme]
----
include::chapter-1/1.2/1.16.scm[]
----

[quote,page 60]
____
In general, the technique of defining an invariant quantity that remains unchanged from state to state is a powerful way to think about the design of iterative algorithms.
____

==== Exercise 1.17

[source,scheme]
----
include::chapter-1/1.2/1.17.scm[]
----

==== Exercise 1.18

[source,scheme]
----
include::chapter-1/1.2/1.18.scm[]
----

==== Exercise 1.19

----
p' = p^2 + q^2
q' = q^2 + 2pq

Applying T(p,q) twice equals to applying T(p', q') once.
----

---

.Euclid’s Algorithm: An elegant and effective algorithm to compute *Greatest Common Divisor*.
[source,scheme]
----
(define (gcd a b) (if (= b 0)
  a
  (gcd b (remainder a b))))
----

[subs="quotes"]
----
*Lamé’s Theorem*: If Euclid’s Algorithm requires k steps to compute the GCD of some pair, then the smaller number in the pair must be greater than or equal to the kth Fibonacci number.
----

Based on Lamé’s Theorem, we can get an order-of-growth estimate for Euclid's Algorithm.

Let `n` be the smaller of the two inputs to the procedure. If the process takes `k` steps, then we must have `n` ≥ Fib(k) ≈ φ^k/√5. Therefore the number of steps `k` grows as the logarithm (to the base φ) of `n`. Hence, the order of growth is `Θ(log n)`.

==== Exercise 1.20

----
Applicative order:

(gcd 206,40)
+1 -> (gcd 40, 6)
+1 -> (gcd 6, 4)
+1 -> (gcd 4, 2)
+1 -> (gcd 2, 0)
-> result

4 times.

Normal order:

(gcd 206 40)
(gcd 40 (remainder 206 40))
+1 (gcd (remainder 206 40) (remainder 40 (remainder 206 40)))
+2 (gcd (remainder 40 (remainder 206 40)) (remainder (remainder 206 40) (remainder 40 (remainder 206 40))))
+4 (gcd
    (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))
    (remainder
      (remainder 40 (remainder 206 40))
      (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))))
+7 (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))
+4 -> result

18 times!
----

---

.Basic primality test procedure
[source,scheme]
----
include::chapter-1/1.2/basic-prime-test.scm[]
----

This basic primality test is based on the fact that if `n` is not prime it must have a divisor less than or equal to `√n`. Consequently, the number of steps required to identify `n` as prime will have order of growth `Θ(√n)`.

And there is a Θ(log n) test called _the Fermat test_ which is based on a result from number theory known as _Fermat's Little Theorem_.

[subs="quotes"]
----
*Fermat’s Little Theorem*: If n is a prime number and a is any positive integer less than n, then a raised to the nth power is congruent to a modulo n.
----

[quote,page 67]
____
Given a number `n`, pick a random number `a` < `n` and compute the remainder of `a^n` modulo `n`. If the result is not equal to `a`, then `n` is certainly not prime. If it is `a`, then chances are good that `n` is prime. ... By trying more and more values of `a`, we can increase our confidence in the result. This algorithm is known as the Fermat test.
____

The core procedure of the Fermat test is one that computes the exponential of a number modulo another number.

.The Fermat primality test
[source,scheme]
----
; Thee reduction steps in the cases where the exponent e is greater than 1
; are based on the fact that, for any integers x, y, and m,
; we can find the remainder of x times y modulo m by computing separately
; the remainders of x modulo m and y modulo m, multiplying these,
; and then taking the remainder of the result modulo m.

(define (expomod base exp m)
  (cond
    ((= exp 0) 1)
    ((even? exp)
     (remainder
       (square (expomod base (/ exp 2) m))
       m))
    (else
      (remainder
        (* base (expomod base (- exp 1) m))
        m))))

(define (fermat-test n)
  (define (try-it a)
    (= (expomod a n n) a))
  (try-it (+ 1 (random (- n 1)))))

(define (fast-prime? n times)
  (cond ((= times 0) true)
        ((fermat-test n) (fast-prime? n (- times 1)))
        (else false)))

(display (fast-prime? 9677 10))
----

The Fermat test has a significant difference from most familiar algorithms, it is a *probabilistic algorithm* and the result is not guaranteed to be correct. But that doesn't mean it is not useful.

[quote,page 69]
____
The Fermat test differs in character from most familiar algorithms, in which one computes an answer that is guaranteed to be correct. Here, the answer obtained is only probably correct. More precisely, if n ever fails the Fermat test, we can be certain that n is not prime. But the fact that n passes the test, while an extremely strong indication, is still not a guarantee that n is prime. What we would like to say is that for any number n, if we perform the test enough times and find that n always passes the test, then the probability of error in our primality test can be made as small as we like.

Unfortunately, this assertion is not quite correct. There **do exist** numbers that fool the Fermat test: numbers n that are not prime and yet have the property that a^n is congruent to a modulo n for all integers a < n. Such numbers are extremely rare, so the Fermat test is quite reliable in practice.

There are *variations of the Fermat test* that cannot be fooled.
____

==== Exercise 1.21

[source,shceme]
----
(display (smallest-divisor 199))
(newline)
; 199

(display (smallest-divisor 1999))
(newline)
; 1999

(display (smallest-divisor 19999))
(newline)
; 7
----

==== Exercise 1.22

* Use `real-time-clock` instead of `runtime` because `runtime` returns seconds which is too big to observe.
* Computers have become so fast, to get meaningful results, we need to test with very large numbers.

[source,scheme]
----
include::chapter-1/1.2/1.22.scm[]
----

----
1000003.***2
1000033.***2
1000037.***2

10000019.***5
10000079.***5
10000103.***5

100000007.***16
100000037.***15
100000039.***14

1000000007.***44
1000000009.***44
1000000021.***44
----

We can see that the timing data basically corresponds to Θ(√n). The more bigger n gets, the better support for the Θ(√n) prediction.
